{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"rajeev-dw9/med_llama_chat\")\n",
    "# print(config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "# print(model)\n",
    "# print(\"---------------------------------------------------------\")\n",
    "# model = PeftModel.from_pretrained(model, \"rajeev-dw9/med_llama\")\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Project_Files/Finetune/Data/sentences.csv\")\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    first_part = ' '.join(words[:-1])\n",
    "    last_word = words[-1]\n",
    "    return first_part, last_word\n",
    "\n",
    "df['text'], df['pred'] = zip(*df['Sentence'].map(split_sentence))\n",
    "df = df.drop(columns=['Sentence'])\n",
    "df = df.head(100)\n",
    "df.size\n",
    "\n",
    "data = df\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Assuming your data is in a CSV file named \"data.csv\"\n",
    "# # data = pd.read_csv(\"data.csv\")\n",
    "# data = data.head(10)\n",
    "\n",
    "# generator = pipeline('text-generation', model='rajeev-dw9/med_llama')\n",
    "\n",
    "# def get_prediction(text):\n",
    "#     print(text)\n",
    "#     generated_text = generator(text, max_length=50, num_return_sequences=1)\n",
    "#     pred = generated_text[0]['generated_text'].split(text)[-1].strip()\n",
    "#     print(pred)\n",
    "#     return pred\n",
    "\n",
    "# # Apply the prediction function to your text data\n",
    "# data['model_pred'] = data['text'].apply(get_prediction)\n",
    "\n",
    "# f1 = f1_score(data['pred'], data['model_pred'], average='weighted')  # You can change the averaging method\n",
    "\n",
    "# print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Project_Files/Finetune/Data/sentences.csv\")\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    first_part = ' '.join(words[:-1])\n",
    "    last_word = words[-1]\n",
    "    return first_part, last_word\n",
    "\n",
    "df['text'], df['pred'] = zip(*df['Sentence'].map(split_sentence))\n",
    "df = df.drop(columns=['Sentence'])\n",
    "df = df.head(100)\n",
    "df.size\n",
    "\n",
    "data = df\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline\n",
    "\n",
    "data = data.head(10)\n",
    "\n",
    "generator = pipeline('text-generation', model='rajeev-dw9/med_llama')\n",
    "\n",
    "def create_prompt(text):\n",
    "    prompt_template = \"\"\"\n",
    "    {}. What gene/protein is realted to this?\n",
    "    \"\"\"\n",
    "    return prompt_template.format(text)\n",
    "def get_prediction(text):\n",
    "    prompt = create_prompt(text)\n",
    "    print(\"|||||||||\",prompt,\"|||||||||\") \n",
    "    generated_text = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    pred = generated_text[0]['generated_text'].split(prompt)[-1].strip()\n",
    "    print(pred)\n",
    "    print(\"|||||||||-----------------------------|||||||||\")\n",
    "    return pred\n",
    "\n",
    "# Apply the prediction function to your text data\n",
    "data['model_pred'] = data['text'].apply(get_prediction)\n",
    "f1 = f1_score(data['pred'], data['model_pred'], average='weighted')  \n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline\n",
    "\n",
    "# data = pd.read_csv(\"data.csv\")\n",
    "data = data.head(10) \n",
    "\n",
    "available_gpus = torch.cuda.device_count()  \n",
    "devices = list(range(available_gpus))  \n",
    "\n",
    "\n",
    "def create_pipeline(device):\n",
    "    return pipeline('text-generation', model='rajeev-dw9/med_llama', device=device)\n",
    "\n",
    "pipelines = [create_pipeline(device) for device in devices]\n",
    "\n",
    "def create_prompt(text):\n",
    "    prompt_template = \"\"\"\n",
    "    \"Given this information: {}. Based on current research and databases, the noted interaction partner for this gene/protein is: \"\n",
    "    \"\"\"\n",
    "    return prompt_template.format(text)\n",
    "\n",
    "\n",
    "def get_predictions(data, pipelines):\n",
    "    preds = []\n",
    "    for i, row in data.iterrows():\n",
    "        pipeline = pipelines[i % len(pipelines)] \n",
    "        prompt = create_prompt(row['text'])\n",
    "        generated_text = pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "        pred = generated_text[0]['generated_text'].split(prompt)[-1].strip()\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "data['model_pred'] = get_predictions(data, pipelines)\n",
    "f1 = f1_score(data['pred'], data['model_pred'], average='weighted') \n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"rajeev-dw9/med_llama\")\n",
    "print(config)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\") #\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "print(model)\n",
    "print(\"---------------------------------------------------------\")\n",
    "model = PeftModel.from_pretrained(model, \"rajeev-dw9/med_llama\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Project_Files/Finetune/Data/sentences.csv\")\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    first_part = ' '.join(words[:-1])\n",
    "    last_word = words[-1]\n",
    "    return first_part, last_word\n",
    "\n",
    "df['text'], df['pred'] = zip(*df['Sentence'].map(split_sentence))\n",
    "df = df.drop(columns=['Sentence'])\n",
    "df = df.head(100)\n",
    "df.size\n",
    "\n",
    "data = df\n",
    "# data = data.head(10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "data = data.head(15)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rajeev-dw9/med_llama\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def create_prompt(text):\n",
    "    # prompt_template = \"\"\"\n",
    "    # {}. Which gene/protein is related to this?\n",
    "    # \"\"\"\n",
    "\n",
    "    # prompt_template = \"\"\"\n",
    "    # Given this information: {}. Based on current research and databases, the noted interaction partner for this gene/protein is: \n",
    "    # \"\"\"\n",
    "\n",
    "    # Relation between PHYHIP and KIF15 is?\n",
    "    # Gene/Protein relation between PHYHIP and KIF15 are related as?\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    {}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    return prompt_template.format(text)\n",
    "def get_prediction(text):\n",
    "    prompt = create_prompt(text)\n",
    "    print(\"Prompt is : \",prompt) \n",
    "    generated_text = generator(prompt, max_length=200, num_return_sequences=1, temperature=0)\n",
    "    pred = generated_text[0]['generated_text'].split(prompt)[-1].strip() \n",
    "    print(\"Answer is: \", pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "data['model_pred'] = data['text'].apply(get_prediction)\n",
    "f1 = f1_score(data['pred'], data['model_pred'], average='weighted') \n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, \"rajeev-dw9/med_llama_chat\", force_download=True, mirror=\"rajeev-dw9/med_llama_chat\")\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"rajeev-dw9/med_llama_chat\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "# print(\"---------------------------------------------------------\")\n",
    "model = PeftModel.from_pretrained(model, \"rajeev-dw9/med_llama_chat\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Project_Files/Finetune/Data/sentences.csv\")\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    first_part = ' '.join(words[:-1])\n",
    "    last_word = words[-1]\n",
    "    return first_part, last_word\n",
    "\n",
    "df['text'], df['pred'] = zip(*df['Sentence'].map(split_sentence))\n",
    "df = df.drop(columns=['Sentence'])\n",
    "df = df.head(100)\n",
    "df.size\n",
    "\n",
    "data = df\n",
    "data = data.head(10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Project_Files/env_llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/Project_Files/env_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1900: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/Project_Files/env_llm/lib/python3.10/site-packages/transformers/modeling_utils.py:2852: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "\n",
    "# MODEL = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "MODEL = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "ADAPTER = 'rajeev-dw9/med_llama'\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL, legacy=False, use_auth_token=HF_TOKEN)\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=HF_TOKEN,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, ADAPTER, torch_dtype=torch.float16, is_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe drug Risperidone is used to treat a variety of mental illnesses, including:\\n\\n* Schizophrenia\\n* Bipolar disorder\\n* Major depressive disorder\\n* Schizoaffective disorder\\n* Brief psychotic disorder\\n\\nRisperidone is also used to treat irritability in autistic disorder.\\n\\nIn terms of proteins, Risperidone is associated with the protein GRIN2A.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Is KKS22 realted to PHYHIP?\n",
    "# ARMC8\n",
    "# Tell me about disease related to PHYHIP.\n",
    "# List all drugs related to plasma cell myeloma.\n",
    "# Tell me any two diseses that are not at all related.\n",
    " \n",
    "prompt = \"\"\"\n",
    "### Instruction\n",
    "What are the disesases that can be treated with the drug Risperidone. Is it used to treate mentall illness? What are the proteins associated with this drug?\n",
    "### Answer\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_tensors = model.generate(\n",
    "        input_ids=tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda(),\n",
    "        max_new_tokens=512\n",
    "    )[0]\n",
    "\n",
    "tokenizer.decode(output_tensors, skip_special_tokens=True).split('### Answer')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences containing both 'drug' and 'cell myeloma':\n",
      "The drug Daratumumab and the disease plasma cell myeloma have a indication.\n",
      "The drug Elotuzumab and the disease plasma cell myeloma have a indication.\n",
      "The drug Carfilzomib and the disease plasma cell myeloma have a indication.\n",
      "The drug Methylprednisolone and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Alprostadil and the disease plasma cell myeloma have a contraindication.\n",
      "The drug Vincristine and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Prednisone and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Thalidomide and the disease plasma cell myeloma have a indication.\n",
      "The drug Prednisolone and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Cyclophosphamide and the disease plasma cell myeloma have a indication.\n",
      "The drug Cortisone acetate and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Dexamethasone and the disease plasma cell myeloma have a indication.\n",
      "The drug Panobinostat and the disease plasma cell myeloma have a indication.\n",
      "The drug Betamethasone and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Etoposide and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Doxorubicin and the disease plasma cell myeloma have a indication.\n",
      "The drug Vardenafil and the disease plasma cell myeloma have a contraindication.\n",
      "The drug Lenalidomide and the disease plasma cell myeloma have a indication.\n",
      "The drug Pomalidomide and the disease plasma cell myeloma have a indication.\n",
      "The drug Bortezomib and the disease plasma cell myeloma have a indication.\n",
      "The drug Tadalafil and the disease plasma cell myeloma have a contraindication.\n",
      "The drug Carmustine and the disease plasma cell myeloma have a indication.\n",
      "The drug Melphalan and the disease plasma cell myeloma have a indication.\n",
      "The drug Isatuximab and the disease plasma cell myeloma have a indication.\n",
      "The drug Sildenafil and the disease plasma cell myeloma have a contraindication.\n",
      "The drug Triamcinolone and the disease plasma cell myeloma have a off-label use.\n",
      "The drug Dexamethasone acetate and the disease plasma cell myeloma have a indication.\n",
      "The drug Plerixafor and the disease plasma cell myeloma have a indication.\n",
      "The drug Ixazomib and the disease plasma cell myeloma have a indication.\n",
      "The disease plasma cell myeloma and the drug Daratumumab have a indication.\n",
      "The disease plasma cell myeloma and the drug Elotuzumab have a indication.\n",
      "The disease plasma cell myeloma and the drug Carfilzomib have a indication.\n",
      "The disease plasma cell myeloma and the drug Methylprednisolone have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Alprostadil have a contraindication.\n",
      "The disease plasma cell myeloma and the drug Vincristine have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Prednisone have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Thalidomide have a indication.\n",
      "The disease plasma cell myeloma and the drug Prednisolone have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Cyclophosphamide have a indication.\n",
      "The disease plasma cell myeloma and the drug Cortisone acetate have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Dexamethasone have a indication.\n",
      "The disease plasma cell myeloma and the drug Panobinostat have a indication.\n",
      "The disease plasma cell myeloma and the drug Betamethasone have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Etoposide have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Doxorubicin have a indication.\n",
      "The disease plasma cell myeloma and the drug Vardenafil have a contraindication.\n",
      "The disease plasma cell myeloma and the drug Lenalidomide have a indication.\n",
      "The disease plasma cell myeloma and the drug Pomalidomide have a indication.\n",
      "The disease plasma cell myeloma and the drug Bortezomib have a indication.\n",
      "The disease plasma cell myeloma and the drug Tadalafil have a contraindication.\n",
      "The disease plasma cell myeloma and the drug Carmustine have a indication.\n",
      "The disease plasma cell myeloma and the drug Melphalan have a indication.\n",
      "The disease plasma cell myeloma and the drug Isatuximab have a indication.\n",
      "The disease plasma cell myeloma and the drug Sildenafil have a contraindication.\n",
      "The disease plasma cell myeloma and the drug Triamcinolone have a off-label use.\n",
      "The disease plasma cell myeloma and the drug Dexamethasone acetate have a indication.\n",
      "The disease plasma cell myeloma and the drug Plerixafor have a indication.\n",
      "The disease plasma cell myeloma and the drug Ixazomib have a indication.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def search_sentences(csv_file, word1, word2):\n",
    "    matching_sentences = []\n",
    "    with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sentence = row['Sentence']\n",
    "            if word1 in sentence and word2 in sentence:\n",
    "                matching_sentences.append(sentence)\n",
    "    return matching_sentences\n",
    "\n",
    "csv_file = '/home/ubuntu/Project_Files/Finetune/Data/sentences.csv'  # Replace 'your_file.csv' with the path to your CSV file\n",
    "# word1 = 'TNFAIP3' \n",
    "word1 ='drug'\n",
    "# word2 = 'lymphosarcoma'\n",
    "word2 = 'cell myeloma'\n",
    "# word1 = 'CXCL12'\n",
    "# word2 = 'migration'\n",
    "\n",
    "\n",
    "matching_sentences = search_sentences(csv_file, word1, word2)\n",
    "\n",
    "if matching_sentences:\n",
    "    print(\"Sentences containing both '{}' and '{}':\".format(word1, word2))\n",
    "    for sentence in matching_sentences:\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"No sentences found containing both '{}' and '{}'.\".format(word1, word2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Project_Files/Finetune/Data/sentences.csv\")\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    first_part = ' '.join(words[:-1])\n",
    "    last_word = words[-1]\n",
    "    return first_part, last_word\n",
    "\n",
    "df['text'], df['pred'] = zip(*df['Sentence'].map(split_sentence))\n",
    "df = df.drop(columns=['Sentence'])\n",
    "df = df.head(100)\n",
    "df.size\n",
    "\n",
    "data = df\n",
    "# data = data.head(10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "data = data.head(15)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rajeev-dw9/med_llama_chat\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def create_prompt(text):\n",
    "    # prompt_template = \"\"\"\n",
    "    # {}. Which gene/protein is related to this?\n",
    "    # \"\"\"\n",
    "    # prompt_template = \"\"\"\n",
    "    # Given this information: {}. Based on current research and databases, the noted interaction partner for this gene/protein is: \n",
    "    # \"\"\"\n",
    "    # Relation between PHYHIP and KIF15 is?\n",
    "    # Gene/Protein relation between PHYHIP and KIF15 are related as?\n",
    "    prompt_template = \"\"\"\n",
    "    {}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    return prompt_template.format(text)\n",
    "def get_prediction(text):\n",
    "    prompt = create_prompt(text)\n",
    "    print(\"Prompt is : \",prompt) \n",
    "    generated_text = generator(prompt, max_length=200, num_return_sequences=1, temperature=1)\n",
    "    pred = generated_text[0]['generated_text'].split(prompt)[-1].strip() \n",
    "    print(pred)\n",
    "    print(\"Answer is: \", pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "data['model_pred'] = data['text'].apply(get_prediction)\n",
    "f1 = f1_score(data['pred'], data['model_pred'], average='weighted') \n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Project_Files/env_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1900: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/Project_Files/env_llm/lib/python3.10/site-packages/transformers/modeling_utils.py:2852: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "\n",
    "# MODEL = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "MODEL = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# ADAPTER = 'rajeev-dw9/med_llama'\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL, legacy=False, use_auth_token=HF_TOKEN)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=HF_TOKEN,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     base_model, ADAPTER, torch_dtype=torch.float16, is_trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAspirin is a nonsteroidal anti-inflammatory drug (NSAID) used to relieve pain, reduce inflammation, and prevent blood clots. Here are some common uses of aspirin:\\n\\n1. Pain relief: Aspirin is commonly used to relieve mild to moderate pain, such as headaches, muscle aches, and menstrual cramps.\\n2. Reducing inflammation: Aspirin can help reduce inflammation and swelling in the body, which can be useful for treating conditions such as arthritis, bursitis, and tendonitis.\\n3. Preventing blood clots: Aspirin can help prevent blood clots from forming, which can reduce the risk of heart attack and stroke.\\n4. Lowering fever: Aspirin can help lower a high fever, which can be useful for treating conditions such as flu, cold, and meningitis.\\n5. Treating heart conditions: Aspirin can be used to treat certain heart conditions, such as unstable angina, and to reduce the risk of heart attack and stroke in people with coronary artery disease.\\n6. Treating migraines: Aspirin can be used to treat migraines, including migraine headaches and migraine-related nausea and vomiting.\\n7. Treating babies: Aspirin can be used to treat fever and pain in babies, but only under the guidance of a pediatrician.\\n8. Treating gout: Aspirin can be used to treat gout, a condition that causes sudden and severe joint pain, tenderness, and swelling.\\n9. Treating osteoarthritis: Aspirin can be used to treat osteoarthritis, a condition that causes joint pain, stiffness, and limited mobility.\\n10. Preventing colon cancer: Some studies suggest that taking aspirin regularly may help prevent colon cancer, although more research is needed to confirm this.\\n\\nIt's important to note that aspirin can have side effects, especially if taken in high doses or for long periods of time. It's important to follow the recommended dosage and talk to your doctor before taking aspirin for any medical condition.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "### Instruction\n",
    "List uses of Aspirin.\n",
    "### Answer\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_tensors = model.generate(\n",
    "        input_ids=tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda(),\n",
    "        max_new_tokens=1024\n",
    "    )[0]\n",
    "\n",
    "tokenizer.decode(output_tensors, skip_special_tokens=True).split('### Answer')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
